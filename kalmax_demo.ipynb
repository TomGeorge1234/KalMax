{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KalMax**[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TomGeorge1234/KalMax/blob/main/kalmax_demo.ipynb)\n",
    "\n",
    "In this script we demonstrate how to use the KalMax library. We will: \n",
    "\n",
    "0. Load/generate some exemplar spiking and trajectory data .\n",
    "1. Fit KDE receptive fields to the spiking data.\n",
    "2. Run inference (decode position from held-out spikes) by: \n",
    "    1. Estimate likelihood maps using the receptive fields. \n",
    "    2. Kalman filter (and smooth) the modes of the likelihood maps (MLE) to estimate the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install KalMax\n",
    "!pip install git+https://github.com/TomGeorge1234/KalMax.git#egg=kalmax[demo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.collections import LineCollection\n",
    "time_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [[243/255,237/255,117/255],[243/255,218/255,117/255],[243/255,182/255,117/255],[247/255,159/255,117/259],[247/255,142/255,117/255],[247/255,108/255,117/255],[247/255,71/255,117/255]])\n",
    "\n",
    "# Kalmax\n",
    "import kalmax\n",
    "from kalmax.KalmanFilter import KalmanFilter, fit_parameters\n",
    "from kalmax.KDE import kde\n",
    "from kalmax.utils import make_simulated_dataset, fit_gaussian_vmap\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Generate / load data\n",
    "\n",
    "The data is are:\n",
    "\n",
    "* $\\mathbf{x}_t \\in \\mathbb{R}^D$ is the position (more generally the \"latent variable\") at time $t$,\n",
    "* $\\mathbf{s}_t \\in \\mathbb{N}^{N_{\\textrm{cells}}}$  is the vector of spike counts for all $N_{\\textrm{cells}}$ neurons at time $t$,\n",
    "\n",
    "for $t = 1, \\ldots, T$. These are concatenated into data arrays: $\\mathbf{X} \\in \\mathbb{R}^{T \\times D}$ and $\\mathbf{S} \\in \\mathbb{N}^{T \\times N_{\\textrm{cells}}} $\n",
    "\n",
    "<img src=\"./figures/display_figures/input_data.png\" width=850>\n",
    "\n",
    "\n",
    "### 0.1 Artifical place cell data\n",
    "For this demo we'll generate some artificial place cell data (using the [RatInABox](https://github.com/RatInABox-Lab/RatInABox) package). The agent moves around a 1 x 1 meter box under a smooth random motion policy. Each place cell has a small Gaussian receptive field determining when the cell spikes as a function of the agent's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALS\n",
    "TIME_MINS = 30 # total amount of data to generate in minutes\n",
    "N_CELLS = 40 # number of cells to simulate\n",
    "FIRING_RATE = 10 # firing rate of the cells\n",
    "\n",
    "# This function generates a simulated dataset of place cells, you can find it in the utils.py file\n",
    "T, X, S = make_simulated_dataset(time_mins = TIME_MINS, \n",
    "                                 n_cells = N_CELLS,\n",
    "                                 firing_rate = FIRING_RATE,) #alternatively you can load some real data here\n",
    "\n",
    "# Print the shapes of the data\n",
    "print(f\"Shapes:\\n  •Time, T {T.shape}\\n  •Trajectory, X: {X.shape}\\n  •Spikes, S: {S.shape}\")\n",
    "\n",
    "# Define some other useful variables\n",
    "DIMS = X.shape[1] # Number of dimensions, should be 2 in this demo \n",
    "N_W = min(10,N_CELLS) # width of multipanel plots \n",
    "N_H = int(np.ceil(N_CELLS/N_W)) # height of multipanel plot\n",
    "DT = T[1] - T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Test-train split\n",
    "We'll split the data into a training set (for fitting the receptive fields) and a test set (for decoding the trajectory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "TEST_MINS = 5 # amount of data to use for testing in minutes\n",
    "\n",
    "# Split data into training and testing\n",
    "split = jnp.argmin(jnp.abs(T - (T[-1] - TEST_MINS * 60)))\n",
    "T_train, T_test = T[:split], T[split:]\n",
    "S_train, S_test = S[:split], S[split:]\n",
    "X_train, X_test = X[:split], X[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Pad the data, calculate it's extent and discretise the domain\n",
    "\n",
    "We generate a grid of position coordinates bins spanning the D-dimensional domain of the latent space. These are the bins over which we will estimate the receptive fields. We flatten them into a list but record the original shape so we can later visualise the receptive fields. (You may adapt this to a D $\\neq$ 2-dimensional latent accordingly)\n",
    "\n",
    "_Note the discretisation is only applied for calculating the receptive fields and not position decoding which is fully-continuous. Also note we use `'ij'` indexing for making the grid with `np.meshgrid` which is not the default but generalises better to higher dimensional settings. It puts `x` along the first axis and `y` along the second, so we will need to transpose and reverse the ratemaps before plotting._\n",
    "\n",
    "* `bin_positions` is an array of positions shape `(N_bins, D)` spanning the full domain of the data (plus a little bit). \n",
    "* `bin_shape` is a tuple `(n_x_bins, n_y_bins)` for reshapping the grid_positions array into a grid.\n",
    "\n",
    "<img src=\"./figures/display_figures/env_discretisation.png\" width=1200>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "DX = 0.02 # resolution of receptive field estimates in meters\n",
    "PAD = 0.02 # padding around the trajectory in meters\n",
    "\n",
    "# Get max and min data positions\n",
    "min_x, max_x = X_train[:,0].min() - PAD, X_train[:,0].max() + PAD\n",
    "min_y, max_y = X_train[:,1].min() - PAD, X_train[:,1].max() + PAD\n",
    "env_extent = (min_x, max_x, min_y, max_y)\n",
    "\n",
    "\n",
    "# Make a grid of positions to evaluate the receptive fields\n",
    "x = jnp.arange(min_x, max_x+DX, DX) \n",
    "y = jnp.arange(min_y, max_y+DX, DX)\n",
    "bin_coords = jnp.stack(jnp.meshgrid(x, y, indexing='ij'), axis=0) # X is the first dimension, Y is the second dimension (for plotting)\n",
    "bin_positions = jnp.reshape(bin_coords, (DIMS, -1)).T; # x is first coordinate, y is second coordinate\n",
    "bin_shape = bin_coords.shape[1:] # so we can reshape the receptive fields later\n",
    "\n",
    "print(f\"Environment created from data x = {min_x:.2f} to {max_x:.2f} y = {min_y:.2f} to {max_y:.2f}. The resolution is {DX:.2f} meters and discretised shape is (n_x_bins, n_y_bins) = {bin_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4.1 Plot the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6)) \n",
    "lc = LineCollection(np.concatenate([X_train[:-1,None], X_train[1:,None]], axis=1), cmap=time_cmap, norm=plt.Normalize(0, T_train[-1]), linewidth=1)\n",
    "lc.set_array(T_train)\n",
    "lc.set_alpha(0.5)\n",
    "ax.add_collection(lc)\n",
    "ax.scatter(X_train[:,0], X_train[:,1], c=time_cmap(T_train/T_train[-1]),s=6,linewidth=0, alpha=0.5)\n",
    "ax.set_xlim(min_x, max_x); ax.set_ylim(min_y, max_y); ax.set_aspect('equal', 'box')\n",
    "ax.set_title(\"Trajectory\");\n",
    "fig.savefig(\"figures/trajectory.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4.2 Plot the spikes against the trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(N_W*3 + 1, N_H*3))\n",
    "for i in range(N_CELLS):\n",
    "\n",
    "    # Get spike positions\n",
    "    x_spike_positions = X_train[:,0][S_train[:,i] > 0]\n",
    "    y_spike_positions = X_train[:,1][S_train[:,i] > 0]\n",
    "    colors = time_cmap(T_train[S_train[:,i] > 0]/T_train[-1])\n",
    "\n",
    "    # Plot the spikes\n",
    "    ax = fig.add_subplot(N_H, N_W, i+1, xticks=[], yticks=[])\n",
    "    ax.scatter(x_spike_positions, y_spike_positions, s=6, linewidth=0, alpha=0.7, c=colors)\n",
    "    ax.set_title(f\"Cell {i}\")\n",
    "    ax.set_xlim(min_x, max_x); ax.set_ylim(min_y, max_y); ax.set_aspect('equal', 'box')\n",
    "    if i == 0:\n",
    "        ax.set_xticks([min_x.round(1), max_x.round(1)]); ax.set_yticks([min_y.round(1), max_y.round(1)])\n",
    "        \n",
    "fig.suptitle(\"Spike rasters\", y=0.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **KDE** to fit receptive fields \n",
    "\n",
    "We will use **kernel density estimation** to fit the receptive fields. KDE is a way of converting discrete events to smooth functions.\n",
    "\n",
    "Let $\\mathbf{f}(\\mathbf{x}) = [f_1(\\mathbf{x}), \\cdots f_N(\\mathbf{x})]^{\\mathsf{T}}$ be the vector of _expected_ spike counts for each neuron at position $\\mathbf{x}$ (i.e. the receptive fields) which we want to estimate from the data.\n",
    "\n",
    "\n",
    "The KDE estimate of the receptive field for neuron $i$ is:\n",
    "$$f_i(\\mathbf{x}) = \\frac{\\sum_{t_s} K(\\mathbf{x}, \\mathbf{x}_{t_s^{(i)}}) }{\\sum_{t} K(\\mathbf{x}, \\mathbf{x}_{t})}$$\n",
    "where $t_s^{(i)}$ are the times when neuron $i$ spiked.\n",
    "The basic idea is that the _expected_ number of spikes is the no. of spikes at that location x divided by the total no. of visits to that location. \n",
    "\n",
    "$K(\\mathbf{x}, \\mathbf{x}^{\\prime})$ is any kernel function. We will use a Gaussian with covariance $\\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I}$:\n",
    "\n",
    "$$K_{\\textrm{Gaussian}}(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\frac{1}{(2\\pi)^{D/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\mathbf{x}^{\\prime})^{\\mathsf{T}} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{x}^{\\prime})\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate the receptive fields by passing the bins, the trajectory and the spikes and a kernel into the `KDE.kde()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "KERNEL = kalmax.kernels.gaussian_kernel # the kernel function - you can write your own but it must follow a certain signature\n",
    "KERNEL_BANDWIDTH = 0.02 # width of the smoothing kernel in meters\n",
    "\n",
    "# Calculate the firing rate of all cells at all grid positions\n",
    "firing_rate = kde(\n",
    "    bins = bin_positions,\n",
    "    trajectory = X_train,\n",
    "    spikes = S_train,\n",
    "    kernel = KERNEL,\n",
    "    kernel_bandwidth = KERNEL_BANDWIDTH,\n",
    ")\n",
    "\n",
    "# Reshape into receptive fields and divide by DT to get expected spikes per second (not per time bin) \n",
    "firing_rate_maps = firing_rate.reshape((N_CELLS,)+bin_shape) / DT \n",
    "\n",
    "print(f\"The maximum measured firing rate across all cells is {firing_rate_maps.max():.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Plot the fitted receptive fields \n",
    "Remember to reshape the receptive fields back to the original grid shape for plotting. This has x increasing on axis 0 and y increasing on axis 1 so we need to transpose and reverse the receptive fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(N_W*3, N_H*3))\n",
    "for i in range(N_CELLS):\n",
    "\n",
    "    # Get the firing rate map\n",
    "    rate_map = firing_rate_maps[i].T[::-1,:] # flip and reverses so axes x-dim is left to right and y-dim is bottom to top\n",
    "\n",
    "    # Plot the firing rate map\n",
    "    ax = fig.add_subplot(N_H, N_W, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(rate_map, extent=env_extent, cmap='inferno', vmin=0)\n",
    "    ax.set_title(f\"Cell {i}\")\n",
    "    ax.set_xlim(min_x, max_x); ax.set_ylim(min_y, max_y); ax.set_aspect('equal', 'box')\n",
    "    if i == 0:\n",
    "        ax.set_xticks([min_x.round(1), max_x.round(1)]); ax.set_yticks([min_y.round(1), max_y.round(1)])\n",
    "fig.suptitle(\"KDE receptive fields\");\n",
    "fig.savefig(\"figures/receptive_fields.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Kalman filter** to decode position from test spikes\n",
    "\n",
    "...using these receptive fields (fitted on the training data) to decode position from test spikes. There are two steps: \n",
    "\n",
    "1. From the spikes calculate the likelihood maps.\n",
    "2. Fit Gaussians to these. \n",
    "2. Use these the mean and variances of these Gaussians as the state observations in the Kalman filter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calculate likelihoods maps\n",
    "\n",
    "To calculate the likelihoods we use the `poisson_log_likelihood` function. This takes in the spiking data (shape `(T, N_cells)`), the firing rate maps (shape `(N_cells, N_bins)`) and returns the log-likelihoods (shape `(T, N_bins)`) of the spikes at each time step and each position bin.\n",
    "\n",
    "The likelihood of observing $s$ spikes in a time bin given the expected number of spikes $f$ is:\n",
    "\n",
    "$$P(s | f) = \\frac{f^s e^{-f}}{s!}$$\n",
    "\n",
    "so the total log-likelihood of observing the spikes at a position $\\mathbf{x}$ is found by summing the likelihoods over all neurons:\n",
    "\n",
    "$$\\log P(\\mathbf{s} | \\mathbf{z}) = \\sum_{i=1}^{N_\\textrm{cells}} \\log P(s_i | f_i(\\mathbf{z}))$$\n",
    "\n",
    "These can then be reshaped using the `bin_shape` variable to plot the likelihoods as maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kalmax.KDE import poisson_log_likelihood\n",
    "\n",
    "# Calculate the likelihood of the test spikes given the firing rate at each grid position\n",
    "log_likelihoods = poisson_log_likelihood(spikes=S_test, mean_rate=firing_rate) # shape (T, N_bins)\n",
    "likelihoods = jnp.exp(log_likelihoods) # shape (T, N_bins)\n",
    "likelihood_maps = likelihoods.reshape((likelihoods.shape[0],)+bin_shape) # shape (T, N_x_bins, N_y_bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Plot the likelihood maps \n",
    "Coloured dots show the _true_ position of the agent at each timestep. Grey colomaps show the combined likelihood of the observed spikes at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them (only show every 5th time point)\n",
    "fig = plt.figure(figsize=(30, 3))\n",
    "for i in range(0,10):\n",
    "    likelihood_map = likelihood_maps[5*i].T[::-1] # <-- transposes and reverses so y-axis now run from bootm-left to top-left of the array\n",
    "\n",
    "    ax = fig.add_subplot(1, 10, i+1, xticks=[], yticks=[])\n",
    "    im = ax.imshow(likelihood_map, extent=env_extent, cmap='Greys', aspect='equal')\n",
    "    ax.scatter(X_test[5*i,0], X_test[5*i,1], s=15, c=time_cmap(i/10), label=\"True position\")\n",
    "    ax.set_title(f\"{f't = {T_test[5*i]:.1f}s' if i <= 2 else f'{'...' if i == 3 else ''}'}\")\n",
    "    if i == 0:\n",
    "        ax.set_xticks([min_x.round(1), max_x.round(1)]); ax.set_yticks([min_y.round(1), max_y.round(1)])\n",
    "        ax.set_xlabel(\"x position (m)\", labelpad=-5); ax.set_ylabel(\"x position (m)\")\n",
    "cax = fig.add_axes([0.92, 0.2, 0.02, 0.6], frameon=False)\n",
    "fig.colorbar(im, cax=cax, label=\"Log-likelihood (normalised)\");\n",
    "fig.suptitle(r\"log-likelihood maps $\\log P(\\mathbf{y_{t} | \\mathbf{x}})$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fit Gaussians to likelihoods\n",
    "\n",
    "The goal is to approximate these (potentially complex and multimodal) likelihood maps with a Gaussian: \n",
    "\n",
    "$$P(\\mathbf{x}_{t} | \\mathbf{z}) \\approx  A \\exp\\Big[{-\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\mu}_t)^{\\mathsf{T}}\\boldsymbol{\\Sigma}_{t}^{-1}(\\mathbf{z}-\\boldsymbol{\\mu}_t)}\\Big]$$\n",
    "\n",
    "**Why are we doing this?** In the next section we want to use a Kalman filter to _estimate_ position from spiking data. A core assumption of the Kalman model is the \"observations\" of a system are (noisy) linear functions of the state. This is not true (or at least not a very good assumption) for spike counts, instead but probably is for the mode of the likelihood maps may approximately be. \n",
    "\n",
    "_Special case:_ When there are _no_ spikes, the likelihood maps provide almost zero information about the position but the Gaussian fit will still have a constant mean, mode and variance. To prevent this biasing the Kalman filter we will artificially inflate the variance of the Gaussian fit when there are no spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the means, modes and covariances of the fitted Gaussians\n",
    "MLE_means, MLE_modes, MLE_covs = fit_gaussian_vmap(bin_positions, likelihoods)\n",
    "MLE_covs = MLE_covs + 1e6*jnp.eye(DIMS)[None,...] * (S_test.sum(axis=1)==0)[...,None,None] # add large variance to bins where there are no spikes\n",
    "# MLE_means_train, MLE_modes_train, MLE_covs_train = fit_gaussian_vmap(bin_positions, likelihoods_train)\n",
    "\n",
    "for (i,ax) in enumerate(fig.get_axes()[:-1]):\n",
    "    lambda_, v = np.linalg.eig(MLE_covs[5*i])\n",
    "    lambda_ = np.sqrt(lambda_) # convert from variance to standard deviation along the eigenvectors\n",
    "    ell = matplotlib.patches.Ellipse(xy=MLE_modes[5*i],\n",
    "                                     width=lambda_[0]*2, \n",
    "                                     height=lambda_[1]*2,\n",
    "                                     angle=np.rad2deg(np.arctan(v[:, 0][1] / v[:, 0][0])),\n",
    "                                     lw=1, \n",
    "                                     fill=True, \n",
    "                                     edgecolor=time_cmap(i/10),\n",
    "                                     facecolor=matplotlib.colors.to_rgba(time_cmap(i/10), alpha=0.3),\n",
    "                                     label=\"Gaussian fit\")\n",
    "    ax.add_artist(ell)\n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper right')\n",
    "fig.suptitle(r\"Likelihood maps $P(\\mathbf{s_{t} | \\mathbf{z}}) \\approx \\mathcal{N}(\\mathbf{z} ; \\boldsymbol{\\mu}_t, \\boldsymbol{\\Sigma}_t)$\")\n",
    "fig.savefig(\"figures/likelihood_maps_fitted.png\", dpi=300)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inference via Kalman filter\n",
    "Let $\\mathbf{z}_t$ represents the \"state\" of the system (its latent position) and assume it evolves according to a linear dynamical system with Gaussian noise. $\\mathbf{y}_t$ represents an \"observation\" of the system and is a noisy linear function of the state:\n",
    "$$\\mathbf{z}_{t} = \\mathbf{F}_t \\mathbf{z}_{t-1} + \\mathbf{q}_t$$\n",
    "$$\\mathbf{y}_{t} = \\mathbf{H}_t \\mathbf{z}_{t} + \\mathbf{r}_t$$\n",
    "where $\\mathbf{q}_t \\sim \\mathcal{N}(0, \\mathbf{Q}_t)$ and $\\mathbf{r}_t \\sim \\mathcal{N}(0, \\mathbf{R}_t)$ are the process and observation noises respectively. \n",
    "\n",
    "The Kalman filter is an efficient recursive solution to the problem of estimating the posterior distribution of the state given the observations. Since the initial state distribution is Gaussian and the dynamics / observation models are linear with Gaussian noise, the _posterior_ distribution at each time step is also Gaussian and can be computed recursively very fast using the Kalman filter equations (can be found in any textbook, I match the notation to [Chapter 8.2, Probabilistic Machine Learning (Advanced Topics), Kevin Murphy](https://probml.github.io/pml-book/book2.html)). \n",
    "\n",
    "Strictly, Kalman \"filtering\" calculates the posterior distribution of the state given only the observations up to that time. Kalman \"smoothing\" calculates the posterior distribution of the state given all the observations.\n",
    "\n",
    "$$P(\\mathbf{z}_t | \\mathbf{y}_{1:t}) = \\mathcal{N}(\\mathbf{z}_t | \\boldsymbol{\\mu}^{F}_t, \\boldsymbol{\\Sigma}^{F}_t)$$\n",
    "$$P(\\mathbf{z}_t | \\mathbf{y}_{1:T}) = \\mathcal{N}(\\mathbf{z}_t | \\boldsymbol{\\mu}^{S}_t, \\boldsymbol{\\Sigma}^{S}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 KalMax: Kalman smoothing naive Bayes\n",
    "\n",
    "The basic idea here is to combine the benefits of maximum likelihood decoding with Kalman smoothing by using the modes of the likelihood maps as the observations in the Kalman filter. In this manner spikes are not _directly_ used as observations in the Kalman filter bypassing the issue of non-linearity.\n",
    "\n",
    "\n",
    "$$\\mathbf{y}_t = \\mathbf{x}_t^{\\textrm{MLE}} := \\arg\\max_{\\mathbf{x}} P(\\mathbf{s}_t | \\mathbf{x})$$\n",
    "\n",
    "\n",
    "The parameters of the Kalman filters are set as: \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{F} &= \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\\\\n",
    "\\mathbf{Q} &= \\begin{pmatrix} \\sigma_{v}^2 & 0 \\\\ 0 & \\sigma_{v}^2 \\end{pmatrix} \\\\\n",
    "\\mathbf{H} &= \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\\\\n",
    "\\mathbf{R}_t &= \\begin{pmatrix} \\sigma_{o}^2 & 0 \\\\ 0 & \\sigma_{o}^2 \\end{pmatrix} + \\boldsymbol{\\Sigma}_t\n",
    "\\end{align}\n",
    "\n",
    "where $\\sigma_{v} = v_{\\textrm{typical}} \\cdot dt$ and $\\sigma_{o}$ are priors on the speed of the agent and the observation noise respectively. We could \"fit\" these parameters but instead we'll just set them to reasonable values.\n",
    "The observation noise has a fixed term (assume all maximum likelihood estimates are noisy) and a time varying term (assume maximum likelihood estimates with broad likelihoods are _more_ uncertain). This is not a rigorously justified model but it works well in practise: intuitvely it allows the Kalman filter to \"trust\" the likelihood maps more when they are more certain (e.g. because there are more spikes, or the spikes came from more spatially informative neurons).\n",
    "\n",
    "#### 2.4.1 `KalmanFilter` class\n",
    "The `KalmanFilter` has two primary functions: \n",
    "* `KalmanFilter.filter(Y)` takes in the observations $\\mathbf{Y} \\in \\mathbb{R}^{T \\times D_{\\textrm{Y}}}$ and returns the means and covariances of the posterior distribution at each time step.\n",
    "* `KalmanFilter.smooth(mus_f, sigmas_f)` takes in the filtered means ($\\in \\mathbb{R}^{T \\times D}$) and covariances ($\\in \\mathbb{R}^{T \\times D \\times D}$) and returns the _smoothed_ means and covariances of the posterior distribution at each time step.\n",
    "To set the parameters $\\Theta = \\{ \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0, \\mathbf{F}, \\mathbf{Q}, \\mathbf{H}, \\mathbf{R} \\}$ you can either pass them in as arguments to the `KalmanFilter`  initialisation or pass them in at the time of calling the `filter` and `smooth` functions. In the former case they are assumes to be stationary and in the latter case they can be time-varying along an additional 0-th axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors \n",
    "v_typical = 0.5 # typical velocity in m/s (sets the scale of the smoothing prior)\n",
    "sigma_speed = v_typical * DT # (units meters) standard deviation of the velocity prior\n",
    "sigma_obs = 0.25 # (units meters) standard deviation of the fixed observation noise\n",
    "\n",
    "# Fit the parameters\n",
    "mu0 = jnp.array([(max_x-min_x)/2, (max_y-min_y)/2]) # initial mean\n",
    "sigma0 = jnp.eye(DIMS) * 1.0 # initial covariance (very broad)\n",
    "F = jnp.eye(DIMS) # transition matrix\n",
    "Q = jnp.eye(DIMS) * sigma_speed**2 # transition covariance\n",
    "H = jnp.eye(DIMS) # observation matrix\n",
    "R = jnp.eye(DIMS) * sigma_obs**2 # observation covariance\n",
    "\n",
    "# Create the Kalman Filter\n",
    "kalman_filter = KalmanFilter(dim_Z = DIMS, \n",
    "                             dim_Y = DIMS,\n",
    "                             F=F, \n",
    "                             Q=Q, \n",
    "                             H=H,\n",
    "                             R=None, # R is time varying so we don't set it here\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Filtering and smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the observation noise covariance\n",
    "R = MLE_covs + (sigma_obs**2)*jnp.eye(DIMS) \n",
    "\n",
    "# Filter the test observations \n",
    "mus_f, sigmas_f = kalman_filter.filter(\n",
    "                     Y = MLE_modes, # <-- maximum likelihood positions used as observations \n",
    "                     mu0 = mu0,\n",
    "                     sigma0 = sigma0,\n",
    "                     R=R\n",
    "                     )\n",
    "\n",
    "# Smooth the test observations \n",
    "mus_s, sigmas_s = kalman_filter.smooth(\n",
    "                    mus_f = mus_f, \n",
    "                    sigmas_f = sigmas_f)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
    "ax[0].plot(T_test, mus_f[:,0], label=\"x\", color=time_cmap(0.2), alpha=0.7)\n",
    "ax[0].plot(T_test, mus_f[:,1], label=\"y\", color=time_cmap(0.8), alpha=0.7)\n",
    "ax[0].plot(T_test, X_test[:,0], label=\"True\", color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[0].plot(T_test, X_test[:,1], color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[0].set_ylabel(\"position (m)\")\n",
    "ax[0].set_title(f\"Kalman filter, (av. error: {100*jnp.abs(mus_f - X_test).mean():.2f} cm)\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(T_test, mus_s[:,0], label=\"x\", color=time_cmap(0.2), alpha=0.7)\n",
    "ax[1].plot(T_test, mus_s[:,1], label=\"y\", color=time_cmap(0.8), alpha=0.7)\n",
    "ax[1].plot(T_test, X_test[:,0], label=\"True\", color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[1].plot(T_test, X_test[:,1], color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[1].set_ylabel(\"position (m)\")\n",
    "ax[1].set_title(f\"Kalman smoother, (av. error: {100*jnp.abs(mus_s - X_test).mean():.2f} cm)\")\n",
    "ax[1].set_xlabel(\"Time (s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmarks \n",
    "### 3.1 Naive Kalman (benchmark 1)\n",
    "\n",
    "A naive Kalman filter would use the spikes as the observations. \n",
    "\n",
    "$$\\mathbf{y}_{t} = \\mathbf{s}_{t}$$\n",
    "\n",
    "For time-constant dynamics/observations the Kalman filter can be fit by maximum likelihood (exploiting/assuming the fact that $\\mathbf{z}_t$ is known during training). The parameters $\\Theta = \\{\\mathbf{F}, \\mathbf{Q}, \\mathbf{H}, \\mathbf{R}\\}$ which maximise the likelihood of the data under the model $\\mathcal{L}(\\Theta) = \\log P(\\mathbf{Y}, \\mathbf{Z} | \\Theta)$ are found to be:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{F} &= \\Big( \\sum_{t=2}^{T} \\mathbf{z}_{t-1} \\mathbf{z}_{t}^{\\mathsf{T}} \\Big) \\Big( \\sum_{t=2}^{T} \\mathbf{z}_{t-1} \\mathbf{z}_{t-1}^{\\mathsf{T}} \\Big)^{-1} &\\approx \\mathbf{I}\\\\\n",
    "\\mathbf{Q} &= \\frac{1}{T-1} \\sum_{t=2}^{T} (\\mathbf{z}_{t} - \\mathbf{F} \\mathbf{z}_{t-1})(\\mathbf{z}_{t} - \\mathbf{F} \\mathbf{z}_{t-1})^{\\mathsf{T}} &\\approx (v_{\\textrm{typical}}\\cdot dt)^{2}\\mathbf{I}\\\\\n",
    "\\mathbf{H} &= \\Big( \\sum_{t=1}^{T} \\mathbf{z}_{t} \\mathbf{y}_{t}^{\\mathsf{T}} \\Big) \\Big( \\sum_{t=1}^{T} \\mathbf{y}_{t} \\mathbf{y}_{t}^{\\mathsf{T}} \\Big)^{-1} & \\approx \\mathbf{I}\\\\\n",
    "\\mathbf{R} &= \\frac{1}{T} \\sum_{t=1}^{T} (\\mathbf{z}_{t} - \\mathbf{H} \\mathbf{y}_{t})(\\mathbf{z}_{t} - \\mathbf{H} \\mathbf{y}_{t})^{\\mathsf{T}}  & \\approx \\sigma_Y^{2}\\mathbf{I}\n",
    "\\end{align}\n",
    "\n",
    "The most simple possible Kalman filter is one where the spikes are the observations. This is a very simple model - not making use of the receptive fields we just calculated at all - and is unlikely to work well in practice because the spikes are not linear functions of the position. \n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Naive Kalman}:\\\\\n",
    "\\mathbf{z}_{t} = \\mathbf{x}_{t} &\\quad \\textrm{(latent position)}\\\\\n",
    "\\mathbf{y}_{t} = \\mathbf{y}_{t} &\\quad \\textrm{(spikes)}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the parameters \n",
    "mu0, sigma0, F, Q, H, R = fit_parameters(Z = X_train, Y = S_train)\n",
    "\n",
    "# Create the Kalman Filter\n",
    "kalman_filter = KalmanFilter(dim_Z = DIMS, \n",
    "                             dim_Y = N_CELLS,\n",
    "                             F=F, \n",
    "                             Q=Q, \n",
    "                             H=H,\n",
    "                             R=R,\n",
    "                            )\n",
    "\n",
    "# Filter the test observations \n",
    "mus_f, sigmas_f = kalman_filter.filter(\n",
    "                     Y = S_test, # <-- spikes used as observations\n",
    "                     mu0 = mu0,\n",
    "                     sigma0 = sigma0,)\n",
    "\n",
    "# Smooth the test observations \n",
    "mus_s, sigmas_s = kalman_filter.smooth(\n",
    "                    mus_f = mus_f, \n",
    "                    sigmas_f = sigmas_f)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
    "ax[0].plot(T_test, mus_f[:,0], label=\"x\", color=time_cmap(0.2), alpha=0.7)\n",
    "ax[0].plot(T_test, mus_f[:,1], label=\"y\", color=time_cmap(0.8), alpha=0.7)\n",
    "ax[0].plot(T_test, X_test[:,0], label=\"True\", color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[0].plot(T_test, X_test[:,1], color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[0].set_ylabel(\"position (m)\")\n",
    "ax[0].set_title(f\"Kalman filter, (av. error: {100*jnp.abs(mus_f - X_test).mean():.1f} cm)\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(T_test, mus_s[:,0], label=\"x\", color=time_cmap(0.2), alpha=0.7)\n",
    "ax[1].plot(T_test, mus_s[:,1], label=\"y\", color=time_cmap(0.8), alpha=0.7)\n",
    "ax[1].plot(T_test, X_test[:,0], label=\"True\", color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[1].plot(T_test, X_test[:,1], color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax[1].set_ylabel(\"position (m)\")\n",
    "ax[1].set_title(f\"Kalman smoother, (av. error: {100*jnp.abs(mus_s - X_test).mean():.1f} cm)\")\n",
    "ax[1].set_xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Maximum likelihood estimates (benchmark 2)\n",
    "\n",
    "Finally we can plot how well _just_ the maximum likelihood estimates perform (no Kalman filter). This is the simplest possible model and its pretty common in practise. The second issue is that the likelihoods are just very noisy (there aren't many neurons, and their Poisson spikes are only a noisy function of the position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,2), sharex=True)\n",
    "ax.scatter(T_test, MLE_modes[:,0], label=\"x\", color=time_cmap(0.2), alpha=0.7, linewidth=0, s=5)\n",
    "ax.scatter(T_test, MLE_modes[:,1], label=\"y\", color=time_cmap(0.8), alpha=0.7, linewidth=0, s=5)\n",
    "ax.plot(T_test, X_test[:,0], label=\"True\", color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax.plot(T_test, X_test[:,1], color='k', linestyle=\"-\", alpha=0.5)\n",
    "ax.set_ylabel(\"position (m)\")\n",
    "ax.set_title(f\"Naive Bayes, (av. error: {100*jnp.abs(mus_f - X_test).mean():.1f} cm)\")\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
